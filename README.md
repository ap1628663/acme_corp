# acme_corp

1. Produce a Dockerfile for the sample NodeJS app at the provided GitHub repository: Share the Dockerfile produced by you.
- dockerfile

2. Push the image generated by the Dockerfile into a DockerHub account and tag it as “v1.0”: share the DockerHub image repository URL.
- https://hub.docker.com/repository/docker/abhishek2106/sample-node-app/general	
docker pull abhishek2106/sample-node-app:v1.0

3. Create the Deployment YAML File to deploy this image into a Kubernetes cluster.
- deployment.yaml

4. Create a YAML file that generates an Ingress for the cluster.
- ingress.yaml
  
also

Install nginx Ingress Controller as a Custom Resource Definition (CRD).
Step 1: Add the official stable repository
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx

Step 2: Update the repository
helm repo update

Step 3: Install nginx Ingress Controller
"helm install nginx-ingress ingress-nginx/ingress-nginx"

also

Install nginx Ingress Controller for minukube
"minikube addons enable ingress"

5. Provide YAML file to create new namespace in Kubernetes cluster called “monitoring”.
- namespace.yaml

6. Provide YAML file to deploy Prometheus, Grafana, to the “monitoring” namespace
- prometheus_grafana.yaml

also

We can deploy Prometheus and Grafana using Helm charts. 
Step 1:Install and initialized Helm.
Step 2: helm install prometheus stable/prometheus-operator --namespace monitoring
Step 3: helm install grafana stable/grafana --namespace monitoring

7. Let’s say a Kubernetes job should finish in 40 seconds, however on a rare occasion it takes 5 minutes, How can you make sure to stop the application if it exceeds more than 40 seconds?
- We can use livenessProbe and terminationGracePeriodSeconds to handle this scenario.

8. Let’s say the cluster has two nodes, one of which has a GPU, and we want the sample app to be deployed only on the node with the GPU, how can you ensure deployment runs app on that node exclusively? 
- We can use Kubernetes node selectors and affinity/anti-affinity rules.

 *Node Selector: We can assign labels to nodes to identify which node has a GPU. For instance, label the node with GPU as gpu=true and the other node as gpu=false.

 *Pod Definition: In pod manifest, wecan specify a node selector to indicate that the pod should run only on nodes with the GPU. 

 *Affinity/Anti-affinity: We can use affinity/anti-affinity rules to specify that the pod should be scheduled on a node with a GPU. We can use node affinity to ensure that the pod is scheduled onto a node 
 with the specified label.

9. Whenever we deploy container into the cluster it is pulled from DockerHub but what change will have to be made if we need to pull container from alternative source, say Azure Repos?
- If we need to pull containers from remote source instead of DockerHub, we need to modify the container image references in deployment manifest.
  registry.example.com/username/image:tag

10. Describe any logging infrastructure to implement centralized logging.
- I'm choosing GCP infra
 Step1: Enable Cloud Logging
 Step2: Set Up Kubernetes Cluster
 Step3: Install Fluentd Sidecar
 Step4: Configure Fluentd to Collect Logs
 Step5: Deploy Fluentd ConfigMap and Daemonset
 Step6: Set Up Log Monitoring and Alerting

11. We want to give ‘kubectl’ access to developers but only read access to the cluster, how will you configure it?
- Create a role with read-only permissions and RoleBinding bind it to the developers group.

12. Suggest any other architectural components that may make the cluster highly available, reliable.
- Load Balancing
- Replication
- Auto-scaling
- Fault Tolerance
- Data Replication
- Disaster Recovery plan

High Availability for Kubernetes:
- ReplicaSet
- Horizontal Pod Autoscaler (HPA)
- Pod Disruption Budget (PDB)
- Node and Pod Affinity/Anti-Affinity  
- Fault Tolerance and Self-healing - Health Checks, Pod Lifecycle Management, Node Replacement.
- Stateful Workloads
